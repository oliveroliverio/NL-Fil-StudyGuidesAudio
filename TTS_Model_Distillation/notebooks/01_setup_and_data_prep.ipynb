{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TTS Model Distillation: Setup & Data Prep\n",
                "\n",
                "This notebook initializes the workspace and prepares the dataset from the parent project's `output/` directory.\n",
                "\n",
                "## Objectives\n",
                "1.  Verify GPU availability (RTX 6000).\n",
                "2.  Define paths and configuration.\n",
                "3.  Ingest audio/transcript pairs from `../output`.\n",
                "4.  Format data for training (LJSpeech format).\n",
                "5.  Visualize audio quality (Mel Spectrograms)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import pandas as pd\n",
                "import librosa\n",
                "import librosa.display\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append('../src')\n",
                "\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "PARENT_OUTPUT_DIR = Path(\"../output\") # Where the ElevenLabs audio lives\n",
                "DATASET_DIR = Path(\"../datasets/raw\")\n",
                "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Audio Settings\n",
                "TARGET_SR = 22050 # Standard for VITS/XTTS usually, or 24000\n",
                "TRIM_SILENCE = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Ingestion\n",
                "We need to find pairs of `.mp3` (audio) and `.md` (transcript) files. \n",
                "*Note: The parent project currently outputs `transcript.md` and `audio.mp3`. We need to match them.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_data_pairs(source_dir):\n",
                "    data = []\n",
                "    source_path = Path(source_dir)\n",
                "    \n",
                "    # List all mp3 files\n",
                "    audio_files = list(source_path.glob(\"*.mp3\"))\n",
                "    \n",
                "    for audio_file in audio_files:\n",
                "        # Assuming naming convention: name.mp3 and name_transcript.md\n",
                "        # Or we might need to parse the transcript file if it's separate.\n",
                "        # Let's check for a corresponding transcript file.\n",
                "        base_name = audio_file.stem\n",
                "        \n",
                "        # Try to find the transcript. \n",
                "        # In the parent script: output = f\"output/{base_name}.mp3\"\n",
                "        # And transcript was: f\"output/{base_name}_transcript.md\"\n",
                "        \n",
                "        transcript_file = source_path / f\"{base_name}_transcript.md\"\n",
                "        \n",
                "        if transcript_file.exists():\n",
                "            with open(transcript_file, 'r', encoding='utf-8') as f:\n",
                "                text = f.read()\n",
                "            \n",
                "            data.append({\n",
                "                'audio_path': str(audio_file),\n",
                "                'text': text,\n",
                "                'id': base_name\n",
                "            })\n",
                "        else:\n",
                "            print(f\"Warning: No transcript found for {audio_file.name}\")\n",
                "            \n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "df = find_data_pairs(PARENT_OUTPUT_DIR)\n",
                "print(f\"Found {len(df)} pairs.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing & Formatting (LJSpeech Style)\n",
                "Format: `id|text` (Audio files usually in a `wavs` folder)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_dataset(df, output_dir, target_sr=22050):\n",
                "    wavs_dir = output_dir / \"wavs\"\n",
                "    wavs_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    metadata = []\n",
                "    \n",
                "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
                "        # Load Audio\n",
                "        y, sr = librosa.load(row['audio_path'], sr=target_sr)\n",
                "        \n",
                "        # Trim Silence\n",
                "        if TRIM_SILENCE:\n",
                "            y, _ = librosa.effects.trim(y, top_db=20)\n",
                "        \n",
                "        # Save as WAV\n",
                "        out_name = f\"{row['id']}.wav\"\n",
                "        out_path = wavs_dir / out_name\n",
                "        \n",
                "        import soundfile as sf\n",
                "        sf.write(out_path, y, target_sr)\n",
                "        \n",
                "        # Clean Text (Minimal for now, can add phonemization later)\n",
                "        clean_text = row['text'].strip()\n",
                "        \n",
                "        metadata.append(f\"{row['id']}|{clean_text}\")\n",
                "        \n",
                "    # Save Metadata\n",
                "    with open(output_dir / \"metadata.csv\", 'w', encoding='utf-8') as f:\n",
                "        f.write(\"\\n\".join(metadata))\n",
                "        \n",
                "    print(f\"Processed dataset saved to {output_dir}\")\n",
                "\n",
                "if len(df) > 0:\n",
                "    preprocess_dataset(df, DATASET_DIR, target_sr=TARGET_SR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(df) > 0:\n",
                "    # Visualize the last processed file\n",
                "    example_wav = list((DATASET_DIR / \"wavs\").glob(\"*.wav\"))[0]\n",
                "    y, sr = librosa.load(example_wav)\n",
                "    \n",
                "    plt.figure(figsize=(10, 4))\n",
                "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
                "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
                "    librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
                "    plt.colorbar(format='%+2.0f dB')\n",
                "    plt.title('Mel-frequency spectrogram')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}